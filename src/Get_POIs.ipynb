{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get POIs from OSM Data via Overpass API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import overpy\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from shapely.wkt import loads\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/accessibility-analysis-isocalors\n"
     ]
    }
   ],
   "source": [
    "# Change working directory to parent folder\n",
    "os.chdir(\"..\")  # Move up one directory level\n",
    "\n",
    "# Check current working directory\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define function to get POIs from Overpass API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pois(area_name, key, tags, output_name):\n",
    "    \"\"\"\n",
    "    Download and process POIs from OpenStreetMap using Overpass API.\n",
    "    \n",
    "    Parameters:\n",
    "    - area_name (str): Name of the area to query (e.g., \"Heidelberg\").\n",
    "    - key (str): The key for the POIs (e.g., \"amenity\", \"shop\").\n",
    "    - tags (list): List of tag values to query (e.g., [\"hospital\", \"clinic\"]).\n",
    "    - output_name (str): Name of the output GeoJSON file (e.g., \"pois_hd_hospitals_clinics\").\n",
    "    \"\"\"\n",
    "    # Initialize Overpass API\n",
    "    api = overpy.Overpass()\n",
    "    \n",
    "    # Build query for the provided key and tags\n",
    "    tag_queries = \"\\n\".join(\n",
    "        f'node[\"{key}\"=\"{tag}\"](area.searchArea);\\nway[\"{key}\"=\"{tag}\"](area.searchArea);\\nrelation[\"{key}\"=\"{tag}\"](area.searchArea);'\n",
    "        for tag in tags\n",
    "    )\n",
    "    \n",
    "    query = f\"\"\"\n",
    "        [out:json];\n",
    "        area[\"name\"=\"{area_name}\"][\"admin_level\"=\"6\"]->.searchArea;\n",
    "        (\n",
    "          {tag_queries}\n",
    "        );\n",
    "        out center;\n",
    "    \"\"\"\n",
    "    \n",
    "    # Execute query\n",
    "    result = api.query(query)\n",
    "    \n",
    "    # Prepare data for GeoDataFrame\n",
    "    data = []\n",
    "    \n",
    "    for element in result.nodes + result.ways + result.relations:\n",
    "        name = element.tags.get('name', 'N/A')\n",
    "        category = element.tags.get(key, 'N/A')\n",
    "        all_tags = dict(element.tags)  \n",
    "        \n",
    "        if isinstance(element, overpy.Node):\n",
    "            lat, lon = element.lat, element.lon\n",
    "            osm_id = f\"node/{element.id}\"\n",
    "            element_type = 'node'\n",
    "        elif isinstance(element, overpy.Way):\n",
    "            lat, lon = element.center_lat, element.center_lon\n",
    "            osm_id = f\"way/{element.id}\"\n",
    "            element_type = 'way'\n",
    "        else:  # Relation\n",
    "            lat, lon = element.center_lat, element.center_lon\n",
    "            osm_id = f\"relation/{element.id}\"\n",
    "            element_type = 'relation'\n",
    "        \n",
    "        data.append({\n",
    "            'osm_id': osm_id,\n",
    "            'name': name,\n",
    "            'category': category,\n",
    "            'tags': all_tags,\n",
    "            'geom_type': element_type,\n",
    "            'geometry': Point(lon, lat)\n",
    "        })\n",
    "    \n",
    "    # Create GeoDataFrame\n",
    "    gdf = gpd.GeoDataFrame(data, crs=\"EPSG:4326\", geometry='geometry')\n",
    "    gdf['priority'] = gdf['geom_type'].map({'node': 0, 'way': 1, 'relation': 2})\n",
    "    gdf = gdf.sort_values(['name', 'priority'])\n",
    "    \n",
    "    # Filter nearby duplicates\n",
    "    def filter_nearby_duplicates(group):\n",
    "        if len(group) == 1:\n",
    "            return group\n",
    "        \n",
    "        # Convert to a projected CRS for accurate distance calculation\n",
    "        group_projected = group.to_crs(epsg=25832)\n",
    "        \n",
    "        # Create a 100m buffer around the first point\n",
    "        buffer = group_projected.iloc[0].geometry.buffer(100)\n",
    "        \n",
    "        # Select points that are outside this buffer\n",
    "        outside_buffer = group_projected[~group_projected.geometry.within(buffer)]\n",
    "        \n",
    "        # Combine the first point with those outside the buffer\n",
    "        result = pd.concat([group_projected.iloc[[0]], outside_buffer])\n",
    "        \n",
    "        # Convert back to original CRS\n",
    "        return result.to_crs(gdf.crs)\n",
    "    \n",
    "    gdf = gdf.groupby('name', group_keys=False).apply(filter_nearby_duplicates)\n",
    "    gdf = gdf.drop(columns=['priority'])\n",
    "    \n",
    "    # Write the GeoDataFrame to a GeoJSON file\n",
    "    output_path = Path.cwd() / 'data' / f\"{output_name}.geojson\"\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    gdf.to_file(output_path, driver='GeoJSON')\n",
    "    \n",
    "    print(f\"Data saved to {output_path}\")\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nStatistics:\")\n",
    "    stats = gdf['category'].value_counts()\n",
    "    print(stats)\n",
    "    print(f\"\\nTotal number of POIs: {len(gdf)}\")\n",
    "\n",
    "    return gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_111/1385233226.py:87: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  gdf = gdf.groupby('name', group_keys=False).apply(filter_nearby_duplicates)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to /home/jovyan/accessibility-analysis-isocalors/data/pois_hd_supermarket_osm.geojson\n",
      "\n",
      "Statistics:\n",
      "category\n",
      "supermarket    55\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total number of POIs: 55\n",
      "Data saved to /home/jovyan/accessibility-analysis-isocalors/data/pois_hd_health_osm.geojson\n",
      "\n",
      "Statistics:\n",
      "category\n",
      "doctors         175\n",
      "kindergarten    147\n",
      "pharmacy         38\n",
      "hospital         32\n",
      "clinic           10\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total number of POIs: 402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_111/1385233226.py:87: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  gdf = gdf.groupby('name', group_keys=False).apply(filter_nearby_duplicates)\n"
     ]
    }
   ],
   "source": [
    "# Call function for supermarkets\n",
    "gdf_supermarkets = download_pois(\"Heidelberg\", \"shop\", [\"supermarket\"], \"pois_hd_supermarket_osm\")\n",
    "\n",
    "# Call function for hospitals, clinics, and doctors, pharmacies and kindergartens\n",
    "gdf_pois = download_pois(\"Heidelberg\", \"amenity\", [\"hospital\", \"clinic\", \"doctors\", \"pharmacy\", \"kindergarten\"], \"pois_hd_health_osm\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Access transport stops in Heidelberg via overpass API  (bus stops, train stations, tram stops)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to /home/jovyan/accessibility-analysis-isocalors/data/pois_hd_transport_osm.geojson\n",
      "\n",
      "Statistics:\n",
      "category\n",
      "transport    356\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total number of POIs: 356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_111/3254130208.py:83: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  gdf = gdf.groupby('name', group_keys=False).apply(filter_nearby_duplicates)\n"
     ]
    }
   ],
   "source": [
    "# Initialize Overpass API\n",
    "api = overpy.Overpass()\n",
    "\n",
    "# Build Query for Transport Stops in Heidelberg\n",
    "result = api.query(\"\"\"\n",
    "    [out:json];\n",
    "    area[\"name\"=\"Heidelberg\"][\"admin_level\"=\"6\"]->.searchArea;\n",
    "    (\n",
    "      node[\"highway\"=\"bus_stop\"](area.searchArea);\n",
    "      node[\"railway\"=\"halt\"](area.searchArea);\n",
    "      node[\"railway\"=\"station\"](area.searchArea);\n",
    "      node[\"railway\"=\"tram_stop\"](area.searchArea);\n",
    "      way[\"highway\"=\"bus_stop\"](area.searchArea);\n",
    "      way[\"railway\"=\"halt\"](area.searchArea);\n",
    "      way[\"railway\"=\"station\"](area.searchArea);\n",
    "      way[\"railway\"=\"tram_stop\"](area.searchArea);\n",
    "      relation[\"highway\"=\"bus_stop\"](area.searchArea);\n",
    "      relation[\"railway\"=\"halt\"](area.searchArea);\n",
    "      relation[\"railway\"=\"station\"](area.searchArea);\n",
    "      relation[\"railway\"=\"tram_stop\"](area.searchArea);\n",
    "    );\n",
    "    out center;\n",
    "\"\"\")\n",
    "\n",
    "# Prepare data for GeoDataFrame\n",
    "data = []\n",
    "\n",
    "for element in result.nodes + result.ways + result.relations:\n",
    "    name = element.tags.get('name', 'N/A')\n",
    "    category = (\n",
    "        element.tags.get('highway', element.tags.get('railway', 'N/A'))\n",
    "    )\n",
    "    all_tags = dict(element.tags)\n",
    "    \n",
    "    if isinstance(element, overpy.Node):\n",
    "        lat, lon = element.lat, element.lon\n",
    "        osm_id = f\"node/{element.id}\"\n",
    "        element_type = 'node'\n",
    "    elif isinstance(element, overpy.Way):\n",
    "        lat, lon = element.center_lat, element.center_lon\n",
    "        osm_id = f\"way/{element.id}\"\n",
    "        element_type = 'way'\n",
    "    else:  # Relation\n",
    "        lat, lon = element.center_lat, element.center_lon\n",
    "        osm_id = f\"relation/{element.id}\"\n",
    "        element_type = 'relation'\n",
    "    \n",
    "    data.append({\n",
    "        'osm_id': osm_id,\n",
    "        'name': name,\n",
    "        'category': 'transport',\n",
    "        'tags': all_tags,\n",
    "        'geom_type': element_type,\n",
    "        'geometry': Point(lon, lat)\n",
    "    })\n",
    "\n",
    "# Create GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(data, crs=\"EPSG:4326\", geometry='geometry')\n",
    "gdf['priority'] = gdf['geom_type'].map({'node': 0, 'way': 1, 'relation': 2})\n",
    "gdf = gdf.sort_values(['name', 'priority'])\n",
    "\n",
    "# Function to filter nearby duplicates using GeoPandas\n",
    "def filter_nearby_duplicates(group):\n",
    "    if len(group) == 1:\n",
    "        return group\n",
    "    \n",
    "    # Convert to a projected CRS for accurate distance calculation\n",
    "    group_projected = group.to_crs(epsg=25832)\n",
    "    \n",
    "    # Create a 100m buffer around the first point\n",
    "    buffer = group_projected.iloc[0].geometry.buffer(100)\n",
    "    \n",
    "    # Select points that are outside this buffer\n",
    "    outside_buffer = group_projected[~group_projected.geometry.within(buffer)]\n",
    "    \n",
    "    # Combine the first point with those outside the buffer\n",
    "    result = pd.concat([group_projected.iloc[[0]], outside_buffer])\n",
    "    \n",
    "    # Convert back to original CRS\n",
    "    return result.to_crs(gdf.crs)\n",
    "\n",
    "# Apply the filter\n",
    "gdf = gdf.groupby('name', group_keys=False).apply(filter_nearby_duplicates)\n",
    "\n",
    "# Drop the temporary 'priority' column\n",
    "gdf = gdf.drop(columns=['priority'])\n",
    "\n",
    "# Write the GeoDataFrame to a GeoJSON file\n",
    "output_path = Path.cwd() / 'data' / 'pois_hd_transport_osm.geojson'\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "gdf.to_file(output_path, driver='GeoJSON')\n",
    "\n",
    "print(f\"Data saved to {output_path}\")\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\nStatistics:\")\n",
    "stats = gdf['category'].value_counts()\n",
    "print(stats)\n",
    "print(f\"\\nTotal number of POIs: {len(gdf)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Access senior living facilites in Heidelberg via overpass API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to /home/jovyan/accessibility-analysis-isocalors/data/pois_hd_senior_facility_osm.geojson\n",
      "\n",
      "Statistics:\n",
      "category\n",
      "senior_facility    14\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total number of POIs: 14\n"
     ]
    }
   ],
   "source": [
    "# Build query to retrieve data for senior living facilities in Heidelberg from OpenStreetMap\n",
    "api = overpy.Overpass()\n",
    "result = api.query(\"\"\"\n",
    "    [out:json];\n",
    "    area[\"name\"=\"Heidelberg\"][\"admin_level\"=\"6\"]->.searchArea;\n",
    "    (\n",
    "      node[\"amenity\"=\"retirement_home\"](area.searchArea);\n",
    "      way[\"amenity\"=\"retirement_home\"](area.searchArea);\n",
    "      relation[\"amenity\"=\"retirement_home\"](area.searchArea);\n",
    "      node[\"amenity\"=\"nursing_home\"](area.searchArea);\n",
    "      way[\"amenity\"=\"nursing_home\"](area.searchArea);\n",
    "      relation[\"amenity\"=\"nursing_home\"](area.searchArea);\n",
    "      node[\"amenity\"=\"social_facility\"][\"social_facility\"=\"nursing_home\"](area.searchArea);\n",
    "      way[\"amenity\"=\"social_facility\"][\"social_facility\"=\"nursing_home\"](area.searchArea);\n",
    "      relation[\"amenity\"=\"social_facility\"][\"social_facility\"=\"nursing_home\"](area.searchArea);\n",
    "      node[\"amenity\"=\"social_facility\"][\"social_facility\"=\"assisted_living\"](area.searchArea);\n",
    "      way[\"amenity\"=\"social_facility\"][\"social_facility\"=\"assisted_living\"](area.searchArea);\n",
    "      relation[\"amenity\"=\"social_facility\"][\"social_facility\"=\"assisted_living\"](area.searchArea);\n",
    "    );\n",
    "    out center;\n",
    "\"\"\")\n",
    "\n",
    "# Prepare data for GeoDataFrame\n",
    "data = []\n",
    "\n",
    "for element in result.nodes + result.ways + result.relations:\n",
    "    name = element.tags.get('name', 'N/A')\n",
    "    amenity_type = element.tags.get('amenity', 'N/A')\n",
    "    social_facility_type = element.tags.get('social_facility', 'N/A')\n",
    "    all_tags = dict(element.tags)\n",
    "    \n",
    "    if isinstance(element, overpy.Node):\n",
    "        lat, lon = element.lat, element.lon\n",
    "        geom_type = 'node'\n",
    "    elif isinstance(element, overpy.Way):\n",
    "        lat, lon = element.center_lat, element.center_lon\n",
    "        geom_type = 'way'\n",
    "    else:  # Relation\n",
    "        lat, lon = element.center_lat, element.center_lon\n",
    "        geom_type = 'relation'\n",
    "    \n",
    "    data.append({\n",
    "        'osm_id': element.id,\n",
    "        'name': name,\n",
    "        'category': 'senior_facility',\n",
    "        'tags': all_tags, \n",
    "        'geom_type': geom_type,\n",
    "        'geometry': Point(lon, lat)\n",
    "    })\n",
    "\n",
    "# Create GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(data, crs=\"EPSG:4326\")\n",
    "\n",
    "# Write the GeoDataFrame to a GeoJSON file\n",
    "output_path = Path.cwd() / 'data' / 'pois_hd_senior_facility_osm.geojson'\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "gdf.to_file(output_path, driver='GeoJSON')\n",
    "\n",
    "print(f\"Data saved to {output_path}\")\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\nStatistics:\")\n",
    "stats = gdf['category'].value_counts()\n",
    "print(stats)\n",
    "print(f\"\\nTotal number of POIs: {len(gdf)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine added senior living facilities from other sources to the list of senior living facilities**\n",
    "\n",
    "Additional senior living facilities are recovered from [Office for Social Affairs and Seniors of the City of Heidelberg]([https://www.heidelberg.de/site/Heidelberg2021/get/documents_E261654398/heidelberg/Objektdatenbank/50/PDF/50_pdf_wegweiser_senioren_heidelberg.pdf), the [healthcare and career portal kliniken.de](https://www.kliniken.de/altenheim/deutschland/ort/heidelberg), the database of the [federal representation of interests for elderly and care-dependent people](https://www.biva.de/pflege-adressen/stationaer/baden-wuerttemberg/stadt-heidelberg/heidelberg/) and a Google Maps search using the tags ’Seniorenheim’, ’Altenheim’ and ’Pflegeheim’.\n",
    "\n",
    "The additional data is found in the csv file seniorenheime_hd.csv in the data folder of the repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data saved to /home/jovyan/accessibility-analysis-isocalors/data/pois_hd_senior_facility_combined.geojson\n"
     ]
    }
   ],
   "source": [
    "# Path to the CSV file\n",
    "csv_path = Path.cwd() / 'data' / 'seniorenheime_hd.csv'\n",
    "\n",
    "# Load CSV data\n",
    "csv_df = pd.read_csv(csv_path)\n",
    "\n",
    "# Convert WKT column to geometry\n",
    "csv_df['geometry'] = csv_df['WKT'].apply(loads)\n",
    "\n",
    "# Create a GeoDataFrame\n",
    "csv_gdf = gpd.GeoDataFrame(csv_df, geometry='geometry', crs=\"EPSG:4326\")\n",
    "\n",
    "# Rename columns to match the OSM GeoDataFrame schema\n",
    "csv_gdf = csv_gdf.rename(columns={\n",
    "    'Name': 'name',\n",
    "    'Beschreibung': 'tags'\n",
    "})\n",
    "csv_gdf['category'] = 'senior_facility'\n",
    "csv_gdf['osm_id'] = None  # No OSM ID for CSV entries\n",
    "csv_gdf['geom_type'] = 'node'  # Assuming all CSV entries are point geometries\n",
    "\n",
    "# Load the existing GeoJSON data\n",
    "geojson_path = Path.cwd() / 'data' / 'pois_hd_senior_facility_osm.geojson'\n",
    "osm_gdf = gpd.read_file(geojson_path)\n",
    "\n",
    "# Combine both GeoDataFrames\n",
    "combined_gdf = gpd.GeoDataFrame(pd.concat([osm_gdf, csv_gdf], ignore_index=True), crs=\"EPSG:4326\")\n",
    "\n",
    "# Drop WKT column\n",
    "combined_gdf = combined_gdf.drop(columns=['WKT'])\n",
    "\n",
    "# Save the combined GeoDataFrame to a GeoJSON file\n",
    "output_path = Path.cwd() / 'data' / 'pois_hd_senior_facility_combined.geojson'\n",
    "combined_gdf.to_file(output_path, driver='GeoJSON')\n",
    "\n",
    "print(f\"Combined data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine all POIs (except transport) in one dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined GeoJSON saved to: data/pois_hd_osm.geojson\n",
      "\n",
      "Statistics:\n",
      "category\n",
      "doctors            175\n",
      "kindergarten       147\n",
      "supermarket         55\n",
      "pharmacy            38\n",
      "hospital            32\n",
      "senior_facility     31\n",
      "clinic              10\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total number of POIs: 488\n"
     ]
    }
   ],
   "source": [
    "def combine_geojson_files(input_files, output_file):\n",
    "    \"\"\"\n",
    "    Combine multiple GeoJSON files into one GeoJSON file.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_files (list of str): List of paths to input GeoJSON files.\n",
    "    - output_file (str): Path to the output GeoJSON file.\n",
    "    \"\"\"\n",
    "    combined_gdf = gpd.GeoDataFrame()\n",
    "\n",
    "    for file in input_files:\n",
    "        gdf = gpd.read_file(file)\n",
    "        combined_gdf = pd.concat([combined_gdf, gdf], ignore_index=True)\n",
    "    \n",
    "    # Save combined GeoDataFrame to a new GeoJSON file\n",
    "    combined_gdf.to_file(output_file, driver=\"GeoJSON\")\n",
    "    print(f\"Combined GeoJSON saved to: {output_file}\")\n",
    "\n",
    "    # Print statistics\n",
    "    print(\"\\nStatistics:\")\n",
    "    stats = combined_gdf['category'].value_counts()\n",
    "    print(stats)\n",
    "    print(f\"\\nTotal number of POIs: {len(combined_gdf)}\")\n",
    "    \n",
    "\n",
    "# List of input GeoJSON files\n",
    "input_files = [\n",
    "    \"data/pois_hd_supermarket_osm.geojson\",\n",
    "    \"data/pois_hd_health_osm.geojson\",\n",
    "    \"data/pois_hd_senior_facility_combined.geojson\"\n",
    "]\n",
    "\n",
    "# Path for the combined GeoJSON file\n",
    "output_file = \"data/pois_hd_osm.geojson\"\n",
    "\n",
    "# Combine files\n",
    "combine_geojson_files(input_files, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<details>\n",
       "<summary>Click to view session information</summary>\n",
       "<pre>\n",
       "-----\n",
       "geopandas           1.0.1\n",
       "overpy              0.7\n",
       "pandas              2.2.2\n",
       "session_info        1.0.0\n",
       "shapely             2.0.4\n",
       "-----\n",
       "</pre>\n",
       "<details>\n",
       "<summary>Click to view modules imported as dependencies</summary>\n",
       "<pre>\n",
       "anyio                       NA\n",
       "arrow                       1.2.3\n",
       "asttokens                   NA\n",
       "attr                        23.1.0\n",
       "attrs                       23.1.0\n",
       "babel                       2.12.1\n",
       "backcall                    0.2.0\n",
       "brotli                      1.0.9\n",
       "certifi                     2023.07.22\n",
       "charset_normalizer          3.2.0\n",
       "colorama                    0.4.6\n",
       "comm                        0.1.3\n",
       "cython_runtime              NA\n",
       "dateutil                    2.8.2\n",
       "debugpy                     1.6.7\n",
       "decorator                   5.1.1\n",
       "executing                   1.2.0\n",
       "fastjsonschema              NA\n",
       "fqdn                        NA\n",
       "idna                        3.4\n",
       "ipykernel                   6.25.0\n",
       "isoduration                 NA\n",
       "jedi                        0.18.2\n",
       "jinja2                      3.1.2\n",
       "json5                       NA\n",
       "jsonpointer                 2.0\n",
       "jsonschema                  4.18.4\n",
       "jsonschema_specifications   NA\n",
       "jupyter_events              0.6.3\n",
       "jupyter_server              2.7.0\n",
       "jupyterlab_server           2.24.0\n",
       "markupsafe                  2.1.3\n",
       "nbformat                    5.9.1\n",
       "numpy                       1.26.4\n",
       "overrides                   NA\n",
       "packaging                   24.2\n",
       "parso                       0.8.3\n",
       "pexpect                     4.8.0\n",
       "pickleshare                 0.7.5\n",
       "pkg_resources               NA\n",
       "platformdirs                3.9.1\n",
       "prometheus_client           NA\n",
       "prompt_toolkit              3.0.39\n",
       "psutil                      5.9.5\n",
       "ptyprocess                  0.7.0\n",
       "pure_eval                   0.2.2\n",
       "pydev_ipython               NA\n",
       "pydevconsole                NA\n",
       "pydevd                      2.9.5\n",
       "pydevd_file_utils           NA\n",
       "pydevd_plugins              NA\n",
       "pydevd_tracing              NA\n",
       "pygments                    2.15.1\n",
       "pyogrio                     0.9.0\n",
       "pyproj                      3.6.2.dev0\n",
       "pythonjsonlogger            NA\n",
       "pytz                        2023.3\n",
       "referencing                 NA\n",
       "requests                    2.31.0\n",
       "rfc3339_validator           0.1.4\n",
       "rfc3986_validator           0.1.1\n",
       "rpds                        NA\n",
       "send2trash                  NA\n",
       "six                         1.16.0\n",
       "sniffio                     1.3.0\n",
       "socks                       1.7.1\n",
       "stack_data                  0.6.2\n",
       "tornado                     6.3.2\n",
       "traitlets                   5.9.0\n",
       "uri_template                NA\n",
       "urllib3                     2.0.4\n",
       "wcwidth                     0.2.6\n",
       "webcolors                   1.13\n",
       "websocket                   1.6.1\n",
       "yaml                        6.0\n",
       "zmq                         25.1.0\n",
       "zstandard                   0.19.0\n",
       "</pre>\n",
       "</details> <!-- seems like this ends pre, so might as well be explicit -->\n",
       "<pre>\n",
       "-----\n",
       "IPython             8.14.0\n",
       "jupyter_client      8.3.0\n",
       "jupyter_core        5.3.1\n",
       "jupyterlab          4.0.3\n",
       "notebook            7.0.0\n",
       "-----\n",
       "Python 3.11.4 | packaged by conda-forge | (main, Jun 10 2023, 18:08:17) [GCC 12.2.0]\n",
       "Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.35\n",
       "-----\n",
       "Session information updated at 2025-03-14 08:12\n",
       "</pre>\n",
       "</details>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import session_info\n",
    "session_info.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
